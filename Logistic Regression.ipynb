{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1 - gradient descent\\n2 - learning rate, maxiteration, tolerance, X, y\\n3 - add column 0\\n4 - normalize function\\n5 - sigmoid function\\n6 - cost function\\n7 - cost derivative\\n8 - predict function \\n10 - evaluate function\\n    precision\\n    recall\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1 - gradient descent\n",
    "2 - learning rate, maxiteration, tolerance, X, y\n",
    "3 - add column 0\n",
    "4 - normalize function\n",
    "5 - sigmoid function\n",
    "6 - cost function\n",
    "7 - cost derivative\n",
    "8 - predict function \n",
    "10 - evaluate function\n",
    "    precision\n",
    "    recall\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learningRate, maxIteration, tolerance):\n",
    "        self.learningRate = learningRate\n",
    "        self.maxIteration = maxIteration\n",
    "        self.tolerance = tolerance\n",
    "        self.train_X, self.test_X, self.train_y, self.test_y = self.readDataset()\n",
    "\n",
    "    def readDataset(self):\n",
    "        train_df = pd.read_excel('Lab3_data.xls', sheet_name = '2004--2005 Data')\n",
    "        test_df = pd.read_excel('Lab3_data.xls', sheet_name = '2004--2007 Data')\n",
    "\n",
    "        train_df, test_df = train_df.values, test_df.values\n",
    "\n",
    "        train_X, train_y = train_df[:, 1:], train_df[:, 0]\n",
    "        test_X, test_y = test_df[:, 1:], test_df[:, 0]\n",
    "        return train_X, test_X, train_y, test_y\n",
    "\n",
    "    def addX0(self, X):\n",
    "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        sig = 1 / (1+ np.exp(-z))\n",
    "        return sig\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        sig = self.sigmoid(X.dot(self.w))\n",
    "        loss = y * np.log(sig) + (1-y) * np.log(1-sig)\n",
    "        cost = - loss.sum()\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        sig = self.sigmoid(X.dot(self.w))\n",
    "        grad = (sig-y).dot(X)\n",
    "        return grad\n",
    "\n",
    "    def gradientDescent(self, X, y):\n",
    "        losses = []\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for i in tqdm(range(self.maxIteration), colour = 'red'):\n",
    "            self.w = self.w - self.learningRate * self.gradient(X, y)\n",
    "            current_loss = self.costFunction(X, y)\n",
    "            diff_loss = np.abs(prev_loss - current_loss)\n",
    "            losses.append(current_loss)\n",
    "\n",
    "            if diff_loss < self.tolerance:\n",
    "                print(\"The model stopped learning\")\n",
    "                break\n",
    "\n",
    "            prev_loss = current_loss\n",
    "\n",
    "        #self.plot_cost(losses) ---- need to find the code and add this later from the lecture\n",
    "\n",
    "    def predict(self, X):\n",
    "        sig = self.sigmoid(X.dot(self.w))\n",
    "        return np.around(sig) \n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        self.w = np.ones(self.train_X.shape[1], dtype = np.float64)\n",
    "        print('Solving using gradient descent')\n",
    "        self.gradientDescent(self.train_X, self.train_y)\n",
    "\n",
    "        print(\"Evaluating the training results\")\n",
    "        y_hat_train = self.predict(self.train_X)\n",
    "\n",
    "        recall, precision, f_score = self.evaluateFunction(self.train_y, y_hat_train)\n",
    "        print(\"The recall of the model was {}\".format(recall))\n",
    "        print(\"The precision of the model was {}\".format(precision))\n",
    "        print(\"The F1 score of the model was {}\".format(f_score))\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateFunction(self, y, y_hat):\n",
    "        y = (y == 1)\n",
    "        y_hat = (y_hat ==1)\n",
    "\n",
    "        precision = (y & y_hat).sum() / y_hat.sum()\n",
    "        recall = (y & y_hat).sum() / y.sum()\n",
    "\n",
    "        f_score = 2 * (precision * recall)/ (precision + recall)\n",
    "\n",
    "        return recall, precision, f_score\n",
    "    \n",
    "    # def plot function\n",
    "    # def plot 3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(tolerance=0.0, learningRate=0.1e-5, maxIteration= 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving using gradient descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[31m          \u001b[0m| 0/10000 [00:00<?, ?it/s]/var/folders/t4/dfv0dtcj5_3_grr325kbtfgr0000gn/T/ipykernel_83887/2214589592.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y * np.log(sig) + (1-y) * np.log(1-sig)\n",
      "/var/folders/t4/dfv0dtcj5_3_grr325kbtfgr0000gn/T/ipykernel_83887/2214589592.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = y * np.log(sig) + (1-y) * np.log(1-sig)\n",
      "100%|\u001b[31m██████████\u001b[0m| 10000/10000 [00:00<00:00, 36652.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the training results\n",
      "The recall of the model was 0.9722222222222222\n",
      "The precision of the model was 0.9210526315789473\n",
      "The F1 score of the model was 0.9459459459459458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c950c89ebfdbfffcf5a6993bf4eb296a3a71ffbec22c76b5cdb99c8943b004ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
